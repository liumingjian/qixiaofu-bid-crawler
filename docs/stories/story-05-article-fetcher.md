# Story 05: æœç‹—å¾®ä¿¡æœç´¢æ¨¡å—

**Story ID**: STORY-05
**å…³è”ä»»åŠ¡**: Task 3.1
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æœ€é«˜ï¼ˆæŠ€æœ¯æŒ‘æˆ˜ï¼‰
**é¢„è®¡æ—¶é•¿**: 4å°æ—¶
**è´Ÿè´£äºº**: -
**çŠ¶æ€**: âœ… å·²å®Œæˆ
**ä¾èµ–**: STORY-04

---

## ğŸ“‹ Storyæè¿°

é€šè¿‡Seleniumè‡ªåŠ¨åŒ–æœç‹—å¾®ä¿¡æœç´¢ï¼Œè·å–"å–ä¸ƒå°æœå…¬ä¼—å·"çš„æ–‡ç« åˆ—è¡¨ï¼ŒåŒ…æ‹¬æ–‡ç« æ ‡é¢˜ã€URLã€å‘å¸ƒæ—¥æœŸã€æ‘˜è¦ç­‰ä¿¡æ¯ã€‚éœ€è¦å¤„ç†éªŒè¯ç å’ŒåŠ¨æ€åŠ è½½ã€‚

---

## ğŸ¯ éªŒæ”¶æ ‡å‡†

- [x] èƒ½æˆåŠŸæœç´¢å¹¶è¿›å…¥å…¬ä¼—å·ä¸»é¡µ
- [x] èƒ½æ»šåŠ¨åŠ è½½è‡³å°‘50ç¯‡æ–‡ç« 
- [x] è§£æå‡ºæ ‡é¢˜ã€URLã€å‘å¸ƒæ—¥æœŸã€æ‘˜è¦
- [x] é¦–æ¬¡æ‰‹åŠ¨éªŒè¯åä¿å­˜cookies
- [x] åç»­ä½¿ç”¨cookieså…éªŒè¯
- [x] çˆ¬å–æˆåŠŸç‡ â‰¥ 90%

---

## âœ… TODOæ¸…å•

### 1. ç ”ç©¶æœç‹—å¾®ä¿¡æœç´¢ (30åˆ†é’Ÿ)
- [x] è®¿é—® `https://weixin.sogou.com/weixin`
- [x] åˆ†ææœç´¢æµç¨‹:
  - [x] æœç´¢æ¡†å…ƒç´ 
  - [x] æœç´¢ç»“æœé¡µé¢ç»“æ„
  - [x] å…¬ä¼—å·é“¾æ¥
  - [x] å…¬ä¼—å·ä¸»é¡µç»“æ„
  - [x] æ–‡ç« åˆ—è¡¨å…ƒç´ 
  - [x] æ»šåŠ¨åŠ è½½è§¦å‘
- [x] åˆ†æåçˆ¬è™«æœºåˆ¶:
  - [x] éªŒè¯ç ç±»å‹
  - [x] éªŒè¯ç è§¦å‘æ¡ä»¶
  - [x] Cookieç­–ç•¥
- [x] è®°å½•å…³é”®CSSé€‰æ‹©å™¨å’ŒXPath

**è¾“å‡º**: åˆ†ææ–‡æ¡£ï¼Œè®°å½•å…³é”®å…ƒç´ é€‰æ‹©å™¨

---

### 2. åˆ›å»ºæ–‡ç« è·å–ç±» (30åˆ†é’Ÿ)
- [x] åˆ›å»º `core/article_fetcher.py` æ–‡ä»¶
- [x] å®ç° `SougouWeChatFetcher` ç±»
- [x] åœ¨ `__init__()` ä¸­:
  - [x] æ¥æ”¶account_nameå‚æ•°
  - [x] æ¥æ”¶configé…ç½®
  - [x] åˆå§‹åŒ–logger
  - [x] è®¾ç½®æœç‹—æœç´¢URL
  - [x] åˆå§‹åŒ–driverä¸ºNone
- [x] å®šä¹‰æ–‡ç« å…ƒæ•°æ®ç»“æ„

**ä»£ç ç¤ºä¾‹**:
```python
import json
import time
from typing import List
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from utils.logger import setup_logger

class SougouWeChatFetcher:
    def __init__(self, config_file='config.json'):
        with open(config_file) as f:
            config = json.load(f)

        self.account_name = config['wechat']['account_name']
        self.max_articles = config['wechat']['max_articles_per_crawl']
        self.config = config['scraper']

        self.base_url = "https://weixin.sogou.com/weixin"
        self.driver = None
        self.cookies_file = "data/sogou_cookies.json"
        self.logger = setup_logger('article_fetcher', config['paths']['log_dir'])
```

**éªŒè¯**: åˆå§‹åŒ–æˆåŠŸ

---

### 3. å®ç°æµè§ˆå™¨åˆå§‹åŒ– (20åˆ†é’Ÿ)
- [x] å®ç° `init_driver()` æ–¹æ³•
- [x] é…ç½®Chromeé€‰é¡¹:
  - [x] é¦–æ¬¡è¿è¡Œä½¿ç”¨ `headless=False`
  - [x] æ·»åŠ User-Agent
  - [x] ç¦ç”¨è‡ªåŠ¨åŒ–æ£€æµ‹
- [x] åŠ è½½å·²ä¿å­˜çš„cookies(å¦‚æœå­˜åœ¨)
- [x] æ·»åŠ æ—¥å¿—

**ä»£ç ç¤ºä¾‹**:
```python
def init_driver(self):
    """åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨"""
    chrome_options = Options()

    # é¦–æ¬¡éœ€è¦æ‰‹åŠ¨éªŒè¯,ä¸ä½¿ç”¨headless
    # åç»­å¯ä»¥æ”¹ä¸ºTrue
    if self.config.get('headless', False):
        chrome_options.add_argument('--headless')

    # åçˆ¬è™«è®¾ç½®
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    self.driver = webdriver.Chrome(options=chrome_options)
    self.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })

    self.logger.info("Browser initialized")

    # åŠ è½½cookies
    self._load_cookies()
```

**éªŒè¯**: æµè§ˆå™¨æ­£å¸¸å¯åŠ¨

---

### 4. å®ç°Cookieç®¡ç† (20åˆ†é’Ÿ)
- [x] å®ç° `_save_cookies()` æ–¹æ³•:
  - [x] è·å–å½“å‰cookies
  - [x] ä¿å­˜åˆ°JSONæ–‡ä»¶
- [x] å®ç° `_load_cookies()` æ–¹æ³•:
  - [x] ä»JSONåŠ è½½cookies
  - [x] æ·»åŠ åˆ°driver
- [x] æ·»åŠ æ—¥å¿—

**ä»£ç ç¤ºä¾‹**:
```python
import os

def _save_cookies(self):
    """ä¿å­˜cookies"""
    cookies = self.driver.get_cookies()
    os.makedirs('data', exist_ok=True)
    with open(self.cookies_file, 'w') as f:
        json.dump(cookies, f)
    self.logger.info(f"Cookies saved to {self.cookies_file}")

def _load_cookies(self):
    """åŠ è½½cookies"""
    if os.path.exists(self.cookies_file):
        try:
            # å…ˆè®¿é—®æœç‹—åŸŸå
            self.driver.get(self.base_url)
            time.sleep(1)

            with open(self.cookies_file, 'r') as f:
                cookies = json.load(f)

            for cookie in cookies:
                self.driver.add_cookie(cookie)

            self.logger.info("Cookies loaded")
        except Exception as e:
            self.logger.warning(f"Failed to load cookies: {e}")
```

**éªŒè¯**: Cookiesæ­£å¸¸ä¿å­˜å’ŒåŠ è½½

---

### 5. å®ç°æœç´¢å…¬ä¼—å·åŠŸèƒ½ (40åˆ†é’Ÿ)
- [x] å®ç° `_search_account()` æ–¹æ³•:
  - [x] è®¿é—®æœç‹—å¾®ä¿¡æœç´¢é¦–é¡µ
  - [x] è¾“å…¥å…¬ä¼—å·åç§°
  - [x] ç‚¹å‡»æœç´¢æŒ‰é’®
  - [x] ç­‰å¾…æœç´¢ç»“æœåŠ è½½
  - [x] æŸ¥æ‰¾å…¬ä¼—å·é“¾æ¥
  - [x] è¿”å›å…¬ä¼—å·ä¸»é¡µURL
- [x] å¤„ç†éªŒè¯ç :
  - [x] æ£€æµ‹éªŒè¯ç å‡ºç°
  - [x] æç¤ºç”¨æˆ·æ‰‹åŠ¨éªŒè¯
  - [x] ç­‰å¾…ç”¨æˆ·å®Œæˆ
  - [x] ä¿å­˜cookies
- [x] æ·»åŠ è¯¦ç»†æ—¥å¿—

**ä»£ç ç¤ºä¾‹**:
```python
def _search_account(self) -> str:
    """æœç´¢å…¬ä¼—å·,è¿”å›å…¬ä¼—å·ä¸»é¡µURL"""
    self.logger.info(f"Searching for account: {self.account_name}")

    # è®¿é—®æœç‹—å¾®ä¿¡æœç´¢
    self.driver.get(self.base_url)
    time.sleep(2)

    try:
        # è¾“å…¥å…¬ä¼—å·åç§°
        search_box = self.driver.find_element(By.ID, "query")
        search_box.clear()
        search_box.send_keys(self.account_name)

        # ç‚¹å‡»æœç´¢æŒ‰é’®
        search_btn = self.driver.find_element(By.CLASS_NAME, "swz2")
        search_btn.click()
        time.sleep(3)

        # æ£€æµ‹éªŒè¯ç 
        if self._check_captcha():
            self.logger.warning("Captcha detected, please solve manually")
            input("è¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ,å®ŒæˆåæŒ‰Enterç»§ç»­...")
            self._save_cookies()

        # æŸ¥æ‰¾å…¬ä¼—å·é“¾æ¥
        account_link = WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.PARTIAL_LINK_TEXT, self.account_name))
        )

        account_url = account_link.get_attribute("href")
        self.logger.info(f"Found account URL: {account_url}")

        return account_url

    except Exception as e:
        self.logger.error(f"Search failed: {e}", exc_info=True)
        raise

def _check_captcha(self) -> bool:
    """æ£€æµ‹æ˜¯å¦å‡ºç°éªŒè¯ç """
    try:
        # æ ¹æ®å®é™…é¡µé¢è°ƒæ•´é€‰æ‹©å™¨
        self.driver.find_element(By.ID, "seccodeImage")
        return True
    except:
        return False
```

**éªŒè¯**: èƒ½æˆåŠŸæœç´¢åˆ°å…¬ä¼—å·

---

### 6. å®ç°æ–‡ç« åˆ—è¡¨è·å– (60åˆ†é’Ÿ)
- [x] å®ç° `fetch_article_list()` ä¸»æ–¹æ³•:
  - [x] åˆå§‹åŒ–driver
  - [x] æœç´¢å…¬ä¼—å·
  - [x] è®¿é—®å…¬ä¼—å·ä¸»é¡µ
  - [x] æ»šåŠ¨åŠ è½½æ–‡ç« 
  - [x] è§£ææ–‡ç« åˆ—è¡¨
  - [x] è¿”å›æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
- [x] å®ç° `_scroll_to_load_more()` æ–¹æ³•:
  - [x] æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
  - [x] ç­‰å¾…æ–°å†…å®¹åŠ è½½
  - [x] æ£€æµ‹æ˜¯å¦è¿˜æœ‰æ›´å¤šå†…å®¹
- [x] å®ç° `_parse_articles()` æ–¹æ³•:
  - [x] ä½¿ç”¨BeautifulSoupè§£æ
  - [x] æå–æ ‡é¢˜ã€URLã€æ—¥æœŸã€æ‘˜è¦
  - [x] è¿”å›æ–‡ç« åˆ—è¡¨

**ä»£ç ç¤ºä¾‹**:
```python
def fetch_article_list(self, max_articles=50) -> List[dict]:
    """
    è·å–æ–‡ç« åˆ—è¡¨

    Args:
        max_articles: æœ€å¤§æ–‡ç« æ•°

    Returns:
        List[dict]: æ–‡ç« å…ƒæ•°æ®åˆ—è¡¨
    """
    if not self.driver:
        self.init_driver()

    try:
        # 1. æœç´¢å…¬ä¼—å·
        account_url = self._search_account()

        # 2. è®¿é—®å…¬ä¼—å·ä¸»é¡µ
        self.logger.info("Visiting account homepage")
        self.driver.get(account_url)
        time.sleep(3)

        # 3. æ»šåŠ¨åŠ è½½æ–‡ç« 
        articles = []
        while len(articles) < max_articles:
            # è§£æå½“å‰é¡µé¢
            page_articles = self._parse_articles()
            articles.extend(page_articles)

            # å»é‡
            seen_urls = set()
            unique_articles = []
            for article in articles:
                if article['url'] not in seen_urls:
                    seen_urls.add(article['url'])
                    unique_articles.append(article)
            articles = unique_articles

            self.logger.info(f"Loaded {len(articles)} articles so far")

            # æ»šåŠ¨åŠ è½½æ›´å¤š
            if not self._scroll_to_load_more():
                self.logger.info("No more articles to load")
                break

            time.sleep(2)

        return articles[:max_articles]

    except Exception as e:
        self.logger.error(f"Fetch article list failed: {e}", exc_info=True)
        return []
    finally:
        self.close()

def _scroll_to_load_more(self) -> bool:
    """æ»šåŠ¨åŠ è½½æ›´å¤š,è¿”å›æ˜¯å¦æˆåŠŸåŠ è½½"""
    try:
        # è®°å½•å½“å‰é«˜åº¦
        last_height = self.driver.execute_script("return document.body.scrollHeight")

        # æ»šåŠ¨åˆ°åº•éƒ¨
        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        # æ£€æŸ¥æ˜¯å¦åŠ è½½äº†æ–°å†…å®¹
        new_height = self.driver.execute_script("return document.body.scrollHeight")
        return new_height > last_height

    except Exception as e:
        self.logger.error(f"Scroll failed: {e}")
        return False

def _parse_articles(self) -> List[dict]:
    """è§£æå½“å‰é¡µé¢çš„æ–‡ç« åˆ—è¡¨"""
    articles = []

    try:
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        # æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        article_items = soup.find_all('div', class_='weui_media_box')  # ç¤ºä¾‹é€‰æ‹©å™¨

        for item in article_items:
            try:
                title_tag = item.find('h4', class_='weui_media_title')
                url_tag = title_tag.find('a') if title_tag else None
                date_tag = item.find('p', class_='weui_media_extra_info')

                if url_tag:
                    article = {
                        'title': title_tag.get_text().strip(),
                        'url': url_tag.get('href'),
                        'publish_date': date_tag.get_text().strip() if date_tag else '',
                        'digest': ''  # å¯ä»¥æ·»åŠ æ‘˜è¦æå–
                    }
                    articles.append(article)

            except Exception as e:
                self.logger.warning(f"Parse article item failed: {e}")
                continue

    except Exception as e:
        self.logger.error(f"Parse articles failed: {e}", exc_info=True)

    return articles
```

**éªŒè¯**: èƒ½è·å–è‡³å°‘30ç¯‡æ–‡ç« 

---

### 7. æ·»åŠ èµ„æºç®¡ç† (15åˆ†é’Ÿ)
- [x] å®ç° `__enter__()` å’Œ `__exit__()`
- [x] å®ç° `close()` æ–¹æ³•
- [x] æ·»åŠ æ¸…ç†é€»è¾‘

**ä»£ç ç¤ºä¾‹**:
```python
def __enter__(self):
    self.init_driver()
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.close()

def close(self):
    """å…³é—­æµè§ˆå™¨"""
    if self.driver:
        try:
            self.driver.quit()
            self.logger.info("Browser closed")
        except Exception as e:
            self.logger.error(f"Error closing browser: {e}")
        finally:
            self.driver = None
```

**éªŒè¯**: èµ„æºæ­£å¸¸é‡Šæ”¾

---

### 8. ç¼–å†™å•å…ƒæµ‹è¯• (25åˆ†é’Ÿ)
- [x] åˆ›å»º `tests/test_article_fetcher.py`
- [x] æµ‹è¯•ç”¨ä¾‹1: æµ‹è¯•æœç´¢å…¬ä¼—å·
- [x] æµ‹è¯•ç”¨ä¾‹2: æµ‹è¯•è·å–æ–‡ç« åˆ—è¡¨
- [x] æµ‹è¯•ç”¨ä¾‹3: æµ‹è¯•Cookieä¿å­˜å’ŒåŠ è½½
- [x] æµ‹è¯•ç”¨ä¾‹4: æµ‹è¯•æ»šåŠ¨åŠ è½½

**ä»£ç ç¤ºä¾‹**:
```python
import unittest
from core.article_fetcher import SougouWeChatFetcher

class TestArticleFetcher(unittest.TestCase):
    def test_fetch_article_list(self):
        """æµ‹è¯•è·å–æ–‡ç« åˆ—è¡¨(éœ€è¦æ‰‹åŠ¨éªŒè¯ç )"""
        with SougouWeChatFetcher() as fetcher:
            articles = fetcher.fetch_article_list(max_articles=10)

            self.assertGreater(len(articles), 0)
            self.assertIn('title', articles[0])
            self.assertIn('url', articles[0])
```

**éªŒè¯**: æµ‹è¯•é€šè¿‡(éœ€è¦æ‰‹åŠ¨éªŒè¯ç )

---

## ğŸ“¦ äº¤ä»˜ç‰©

- [x] `core/article_fetcher.py` - æ–‡ç« åˆ—è¡¨è·å–ç±»
- [x] `data/sogou_cookies.json` - Cookieæ–‡ä»¶
- [x] `tests/test_article_fetcher.py` - å•å…ƒæµ‹è¯•
- [x] éªŒè¯ç å¤„ç†è¯´æ˜æ–‡æ¡£

### éªŒè¯ç å¤„ç†è¯´æ˜
1. é¦–æ¬¡è¿è¡Œ `SougouWeChatFetcher` æ—¶è¯·åœ¨ `config.scraper.headless` ä¸­è®¾ç½®ä¸º `false`ï¼Œä»¥ä¾¿å¯ä»¥çœ‹åˆ°çœŸå®æµè§ˆå™¨ç•Œé¢ã€‚
2. æœç´¢è¿‡ç¨‹ä¸­è‹¥æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå‘½ä»¤è¡Œä¼šæç¤ºâ€œCaptcha detectedâ€ï¼Œæ­¤æ—¶è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯ã€‚
3. éªŒè¯å®Œæˆååˆ‡å›ç»ˆç«¯æŒ‰ä¸‹ Enter ç»§ç»­æ‰§è¡Œï¼Œç³»ç»Ÿä¼šç«‹å³è°ƒç”¨ `_save_cookies()` å°†éªŒè¯åçš„ Cookie å†™å…¥ `data/sogou_cookies.json`ã€‚
4. åç»­è¿è¡Œé»˜è®¤ä¼šè‡ªåŠ¨åŠ è½½ `data/sogou_cookies.json`ï¼Œæ— éœ€å†æ¬¡æ‰‹åŠ¨è¾“å…¥éªŒè¯ç ï¼›è‹¥éªŒè¯ç å†æ¬¡è§¦å‘ï¼Œé‡å¤ä¸Šè¿°æ­¥éª¤å³å¯ã€‚

---

## ğŸ§ª æµ‹è¯•æ¸…å•

- [x] æœç´¢å…¬ä¼—å·æˆåŠŸ
- [x] è¿›å…¥å…¬ä¼—å·ä¸»é¡µæˆåŠŸ
- [x] æ»šåŠ¨åŠ è½½æˆåŠŸ
- [x] è§£ææ–‡ç« åˆ—è¡¨æˆåŠŸ
- [x] Cookieä¿å­˜å’ŒåŠ è½½æˆåŠŸ
- [x] è·å–è‡³å°‘30ç¯‡æ–‡ç« 

---

## ğŸ“ å¼€å‘ç¬”è®°

### å…³é”®æ³¨æ„äº‹é¡¹
- é¦–æ¬¡è¿è¡Œå¿…é¡»æ‰‹åŠ¨å®ŒæˆéªŒè¯ç 
- éªŒè¯ç å®Œæˆåç«‹å³ä¿å­˜cookies
- åç»­è¿è¡ŒåŠ è½½cookieså¯å…éªŒè¯
- HTMLç»“æ„å¯èƒ½å˜åŒ–,éœ€è¦çµæ´»è°ƒæ•´é€‰æ‹©å™¨
- æ»šåŠ¨åŠ è½½éœ€è¦è¶³å¤Ÿçš„ç­‰å¾…æ—¶é—´

### è°ƒè¯•æŠ€å·§
- ä½¿ç”¨ `headless=False` è§‚å¯Ÿæµè§ˆå™¨è¡Œä¸º
- æ‰“å° `page_source` æŸ¥çœ‹HTMLç»“æ„
- ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ç¡®è®¤é€‰æ‹©å™¨
- æ·»åŠ  `input("ç»§ç»­...")` æš‚åœè°ƒè¯•

### å¯èƒ½é‡åˆ°çš„é—®é¢˜
1. **é—®é¢˜**: éªŒè¯ç é¢‘ç¹å‡ºç°
   - **è§£å†³**: ä¿å­˜å’ŒåŠ è½½cookies

2. **é—®é¢˜**: æ‰¾ä¸åˆ°å…ƒç´ 
   - **è§£å†³**: æ£€æŸ¥é€‰æ‹©å™¨ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´

3. **é—®é¢˜**: æ»šåŠ¨ä¸è§¦å‘åŠ è½½
   - **è§£å†³**: è°ƒæ•´æ»šåŠ¨æ–¹å¼å’Œç­‰å¾…æ—¶é—´

---

## âœ¨ å®Œæˆæ ‡å‡†

- [x] æ‰‹åŠ¨æµ‹è¯•æˆåŠŸ:
```python
from core.article_fetcher import SougouWeChatFetcher

with SougouWeChatFetcher() as fetcher:
    articles = fetcher.fetch_article_list(max_articles=30)

    print(f"Found {len(articles)} articles")
    for article in articles[:5]:
        print(f"- {article['title']}")
        print(f"  {article['url']}")
```
- [x] è·å–è‡³å°‘30ç¯‡æ–‡ç« 
- [x] Cookiesä¿å­˜å’Œå¤ç”¨æˆåŠŸ

---

## ğŸ“… æ—¶é—´è®°å½•

- **å¼€å§‹æ—¶é—´**:
- **å®Œæˆæ—¶é—´**:
- **å®é™…è€—æ—¶**:
- **å¤‡æ³¨**: é¦–æ¬¡éœ€è¦æ‰‹åŠ¨éªŒè¯ç ,çº¦5-10åˆ†é’Ÿ
